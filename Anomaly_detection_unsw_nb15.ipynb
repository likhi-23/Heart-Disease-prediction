{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 267091,
          "sourceType": "datasetVersion",
          "datasetId": 111554
        }
      ],
      "dockerImageVersionId": 30178,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Anomaly_detection_unsw-nb15",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likhi-23/Heart-disease-prediction/blob/main/Anomaly_detection_unsw_nb15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "V_iHM1i3Jkk6"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='top'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "    \n",
        "<h1 style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:150%\"><b> Table of contents </b></h1>\n",
        "\n",
        " - [**Introduction**](#1)\n",
        "\n",
        " - [**Import and Set Up**](#2)\n",
        "\n",
        " - [**Pre-processing and feature selection**](#3)\n",
        "    \n",
        " - [**Modelling and Evaluation**](#4)\n",
        "    - [**Logistical Classification**](#4_1)\n",
        "    - [**kNN**](#4_2)\n",
        "    - [**Decision Tree**](#4_3)\n",
        "    - [**Extra Trees**](#4_4)\n",
        "    - [**Random Forest**](#4_5)\n",
        "    - [**Gradient Boosting Classifier**](#4_6)\n",
        "    - [**Neural Network MLP**](#4_7)\n",
        "    - [**Neural Network MLP (Keras)**](#4_8)\n",
        "    - [**GRU (Keras)**](#4_9)\n",
        "    - [**LSTM (Keras)**](#4_10)\n",
        "    \n",
        " - [**Evaluate**](#5)\n",
        " - [**Federated Learning**](#6)\n",
        "   "
      ],
      "metadata": {
        "id": "8f0sRujLJkk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='2'></a>\n",
        "# <p style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%\"> <b>Import and Set up</b>"
      ],
      "metadata": {
        "id": "BCOWyP3FJklA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from importlib import reload\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import warnings\n",
        "\n",
        "# Configure Jupyter Notebook\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "# pd.set_option('max_colwidth', -1)\n",
        "display(HTML(\"<style>div.output_scroll { height: 35em; }</style>\"))\n",
        "\n",
        "reload(plt)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format ='retina'\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# configure plotly graph objects\n",
        "pio.renderers.default = 'iframe'\n",
        "# pio.renderers.default = 'vscode'\n",
        "\n",
        "pio.templates[\"ck_template\"] = go.layout.Template(\n",
        "    layout_colorway = px.colors.sequential.Viridis,\n",
        "#     layout_hovermode = 'closest',\n",
        "#     layout_hoverdistance = -1,\n",
        "    layout_autosize=False,\n",
        "    layout_width=800,\n",
        "    layout_height=600,\n",
        "    layout_font = dict(family=\"Calibri Light\"),\n",
        "    layout_title_font = dict(family=\"Calibri\"),\n",
        "    layout_hoverlabel_font = dict(family=\"Calibri Light\"),\n",
        "#     plot_bgcolor=\"white\",\n",
        ")\n",
        "\n",
        "# pio.templates.default = 'seaborn+ck_template+gridon'\n",
        "pio.templates.default = 'ck_template+gridon'\n",
        "# pio.templates.default = 'seaborn+gridon'\n",
        "# pio.templates"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:05.316259Z",
          "iopub.execute_input": "2025-01-21T04:33:05.316672Z",
          "iopub.status.idle": "2025-01-21T04:33:07.429671Z",
          "shell.execute_reply.started": "2025-01-21T04:33:05.316554Z",
          "shell.execute_reply": "2025-01-21T04:33:07.428656Z"
        },
        "trusted": true,
        "id": "HQncylkkJklD",
        "outputId": "a62d05a3-5113-4e8b-9977-3237d79caef5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>div.output_scroll { height: 35em; }</style>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\n",
        "df2 = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv')\n",
        "df = pd.concat([df1,df2], ignore_index=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:07.431144Z",
          "iopub.execute_input": "2025-01-21T04:33:07.431491Z",
          "iopub.status.idle": "2025-01-21T04:33:09.371445Z",
          "shell.execute_reply.started": "2025-01-21T04:33:07.431446Z",
          "shell.execute_reply": "2025-01-21T04:33:09.370316Z"
        },
        "trusted": true,
        "id": "ekGeKnMsJklE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:09.373547Z",
          "iopub.execute_input": "2025-01-21T04:33:09.373842Z",
          "iopub.status.idle": "2025-01-21T04:33:09.475179Z",
          "shell.execute_reply.started": "2025-01-21T04:33:09.373804Z",
          "shell.execute_reply": "2025-01-21T04:33:09.474045Z"
        },
        "trusted": true,
        "id": "BS7e7tPOJklG",
        "outputId": "74333492-492a-4a5f-f2e4-effd696c3175"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 257673 entries, 0 to 257672\nData columns (total 45 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   id                 257673 non-null  int64  \n 1   dur                257673 non-null  float64\n 2   proto              257673 non-null  object \n 3   service            257673 non-null  object \n 4   state              257673 non-null  object \n 5   spkts              257673 non-null  int64  \n 6   dpkts              257673 non-null  int64  \n 7   sbytes             257673 non-null  int64  \n 8   dbytes             257673 non-null  int64  \n 9   rate               257673 non-null  float64\n 10  sttl               257673 non-null  int64  \n 11  dttl               257673 non-null  int64  \n 12  sload              257673 non-null  float64\n 13  dload              257673 non-null  float64\n 14  sloss              257673 non-null  int64  \n 15  dloss              257673 non-null  int64  \n 16  sinpkt             257673 non-null  float64\n 17  dinpkt             257673 non-null  float64\n 18  sjit               257673 non-null  float64\n 19  djit               257673 non-null  float64\n 20  swin               257673 non-null  int64  \n 21  stcpb              257673 non-null  int64  \n 22  dtcpb              257673 non-null  int64  \n 23  dwin               257673 non-null  int64  \n 24  tcprtt             257673 non-null  float64\n 25  synack             257673 non-null  float64\n 26  ackdat             257673 non-null  float64\n 27  smean              257673 non-null  int64  \n 28  dmean              257673 non-null  int64  \n 29  trans_depth        257673 non-null  int64  \n 30  response_body_len  257673 non-null  int64  \n 31  ct_srv_src         257673 non-null  int64  \n 32  ct_state_ttl       257673 non-null  int64  \n 33  ct_dst_ltm         257673 non-null  int64  \n 34  ct_src_dport_ltm   257673 non-null  int64  \n 35  ct_dst_sport_ltm   257673 non-null  int64  \n 36  ct_dst_src_ltm     257673 non-null  int64  \n 37  is_ftp_login       257673 non-null  int64  \n 38  ct_ftp_cmd         257673 non-null  int64  \n 39  ct_flw_http_mthd   257673 non-null  int64  \n 40  ct_src_ltm         257673 non-null  int64  \n 41  ct_srv_dst         257673 non-null  int64  \n 42  is_sm_ips_ports    257673 non-null  int64  \n 43  attack_cat         257673 non-null  object \n 44  label              257673 non-null  int64  \ndtypes: float64(11), int64(30), object(4)\nmemory usage: 88.5+ MB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:09.476423Z",
          "iopub.execute_input": "2025-01-21T04:33:09.476671Z",
          "iopub.status.idle": "2025-01-21T04:33:09.520987Z",
          "shell.execute_reply.started": "2025-01-21T04:33:09.476638Z",
          "shell.execute_reply": "2025-01-21T04:33:09.51991Z"
        },
        "trusted": true,
        "id": "84q-bx8MJklH",
        "outputId": "3abc154a-493d-47cc-ed42-9dd7f0e08a2b"
      },
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id       dur proto service state  spkts  dpkts  sbytes  dbytes          rate  sttl  dttl         sload  dload  sloss  dloss     sinpkt  dinpkt  sjit  djit  swin  stcpb  dtcpb  dwin  tcprtt  synack  ackdat  smean  dmean  trans_depth  response_body_len  ct_srv_src  ct_state_ttl  ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports attack_cat  label\n0   1  0.000011   udp       -   INT      2      0     496       0   90909.09020   254     0  1.803636e+08    0.0      0      0      0.011     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    248      0            0                  0           2             2           1                 1                 1               2             0           0                 0           1           2                0     Normal      0\n1   2  0.000008   udp       -   INT      2      0    1762       0  125000.00030   254     0  8.810000e+08    0.0      0      0      0.008     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    881      0            0                  0           2             2           1                 1                 1               2             0           0                 0           1           2                0     Normal      0\n2   3  0.000005   udp       -   INT      2      0    1068       0  200000.00510   254     0  8.544000e+08    0.0      0      0      0.005     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    534      0            0                  0           3             2           1                 1                 1               3             0           0                 0           1           3                0     Normal      0\n3   4  0.000006   udp       -   INT      2      0     900       0  166666.66080   254     0  6.000000e+08    0.0      0      0      0.006     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    450      0            0                  0           3             2           2                 2                 1               3             0           0                 0           2           3                0     Normal      0\n4   5  0.000010   udp       -   INT      2      0    2126       0  100000.00250   254     0  8.504000e+08    0.0      0      0      0.010     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0   1063      0            0                  0           3             2           2                 2                 1               3             0           0                 0           2           3                0     Normal      0\n5   6  0.000003   udp       -   INT      2      0     784       0  333333.32150   254     0  1.045333e+09    0.0      0      0      0.003     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    392      0            0                  0           2             2           2                 2                 1               2             0           0                 0           2           2                0     Normal      0\n6   7  0.000006   udp       -   INT      2      0    1960       0  166666.66080   254     0  1.306667e+09    0.0      0      0      0.006     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    980      0            0                  0           2             2           2                 2                 1               2             0           0                 0           2           2                0     Normal      0\n7   8  0.000028   udp       -   INT      2      0    1384       0   35714.28522   254     0  1.977143e+08    0.0      0      0      0.028     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0    692      0            0                  0           3             2           1                 1                 1               3             0           0                 0           1           3                0     Normal      0\n8   9  0.000000   arp       -   INT      1      0      46       0       0.00000     0     0  0.000000e+00    0.0      0      0  60000.688     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0     46      0            0                  0           2             2           2                 2                 2               2             0           0                 0           2           2                1     Normal      0\n9  10  0.000000   arp       -   INT      1      0      46       0       0.00000     0     0  0.000000e+00    0.0      0      0  60000.712     0.0   0.0   0.0     0      0      0     0     0.0     0.0     0.0     46      0            0                  0           2             2           2                 2                 2               2             0           0                 0           2           2                1     Normal      0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dur</th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n      <th>spkts</th>\n      <th>dpkts</th>\n      <th>sbytes</th>\n      <th>dbytes</th>\n      <th>rate</th>\n      <th>sttl</th>\n      <th>dttl</th>\n      <th>sload</th>\n      <th>dload</th>\n      <th>sloss</th>\n      <th>dloss</th>\n      <th>sinpkt</th>\n      <th>dinpkt</th>\n      <th>sjit</th>\n      <th>djit</th>\n      <th>swin</th>\n      <th>stcpb</th>\n      <th>dtcpb</th>\n      <th>dwin</th>\n      <th>tcprtt</th>\n      <th>synack</th>\n      <th>ackdat</th>\n      <th>smean</th>\n      <th>dmean</th>\n      <th>trans_depth</th>\n      <th>response_body_len</th>\n      <th>ct_srv_src</th>\n      <th>ct_state_ttl</th>\n      <th>ct_dst_ltm</th>\n      <th>ct_src_dport_ltm</th>\n      <th>ct_dst_sport_ltm</th>\n      <th>ct_dst_src_ltm</th>\n      <th>is_ftp_login</th>\n      <th>ct_ftp_cmd</th>\n      <th>ct_flw_http_mthd</th>\n      <th>ct_src_ltm</th>\n      <th>ct_srv_dst</th>\n      <th>is_sm_ips_ports</th>\n      <th>attack_cat</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.000011</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>496</td>\n      <td>0</td>\n      <td>90909.09020</td>\n      <td>254</td>\n      <td>0</td>\n      <td>1.803636e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.011</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>248</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.000008</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1762</td>\n      <td>0</td>\n      <td>125000.00030</td>\n      <td>254</td>\n      <td>0</td>\n      <td>8.810000e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.008</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>881</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.000005</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1068</td>\n      <td>0</td>\n      <td>200000.00510</td>\n      <td>254</td>\n      <td>0</td>\n      <td>8.544000e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.005</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>534</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.000006</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>900</td>\n      <td>0</td>\n      <td>166666.66080</td>\n      <td>254</td>\n      <td>0</td>\n      <td>6.000000e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.006</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>450</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.000010</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2126</td>\n      <td>0</td>\n      <td>100000.00250</td>\n      <td>254</td>\n      <td>0</td>\n      <td>8.504000e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.010</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1063</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0.000003</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>784</td>\n      <td>0</td>\n      <td>333333.32150</td>\n      <td>254</td>\n      <td>0</td>\n      <td>1.045333e+09</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>392</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0.000006</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1960</td>\n      <td>0</td>\n      <td>166666.66080</td>\n      <td>254</td>\n      <td>0</td>\n      <td>1.306667e+09</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.006</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>980</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0.000028</td>\n      <td>udp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1384</td>\n      <td>0</td>\n      <td>35714.28522</td>\n      <td>254</td>\n      <td>0</td>\n      <td>1.977143e+08</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.028</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>692</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0.000000</td>\n      <td>arp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>1</td>\n      <td>0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60000.688</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>arp</td>\n      <td>-</td>\n      <td>INT</td>\n      <td>1</td>\n      <td>0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60000.712</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:09.522447Z",
          "iopub.execute_input": "2025-01-21T04:33:09.522735Z",
          "iopub.status.idle": "2025-01-21T04:33:10.11032Z",
          "shell.execute_reply.started": "2025-01-21T04:33:09.522698Z",
          "shell.execute_reply": "2025-01-21T04:33:10.109076Z"
        },
        "trusted": true,
        "id": "UhHdF5KhJklJ",
        "outputId": "bb70b218-2bb9-4270-9199-8b0c17e70a8c"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                   id            dur   proto service   state          spkts          dpkts        sbytes        dbytes          rate           sttl           dttl         sload         dload          sloss          dloss         sinpkt         dinpkt          sjit           djit           swin         stcpb         dtcpb           dwin         tcprtt         synack         ackdat          smean          dmean    trans_depth  response_body_len     ct_srv_src   ct_state_ttl     ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm   is_ftp_login     ct_ftp_cmd  ct_flw_http_mthd     ct_src_ltm     ct_srv_dst  is_sm_ips_ports attack_cat          label\ncount   257673.000000  257673.000000  257673  257673  257673  257673.000000  257673.000000  2.576730e+05  2.576730e+05  2.576730e+05  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  2.576730e+05  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000       2.576730e+05  257673.000000  257673.000000  257673.000000     257673.000000     257673.000000   257673.000000  257673.000000  257673.000000     257673.000000  257673.000000  257673.000000    257673.000000     257673  257673.000000\nunique            NaN            NaN     133      13      11            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN         10            NaN\ntop               NaN            NaN     tcp       -     FIN            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN     Normal            NaN\nfreq              NaN            NaN  123041  141321  117164            NaN            NaN           NaN           NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN           NaN            NaN            NaN           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                NaN            NaN            NaN            NaN               NaN               NaN             NaN            NaN            NaN               NaN            NaN            NaN              NaN      93000            NaN\nmean     72811.823858       1.246715     NaN     NaN     NaN      19.777144      18.514703  8.572952e+03  1.438729e+04  9.125391e+04     180.000931      84.754957  7.060869e+07  6.582143e+05       4.889317       6.743691     912.300834      98.915462  5.419373e+03     582.251456     121.753661  1.006120e+09  1.002295e+09     119.254629       0.046038       0.023652       0.022386     137.639027     121.649703       0.102242       1.968900e+03       9.383176       1.324978       6.050467          5.238271          4.032677        8.322964       0.012819       0.012850          0.132005       6.800045       9.121049         0.014274        NaN       0.639077\nstd      48929.917641       5.974305     NaN     NaN     NaN     135.947152     111.985965  1.737739e+05  1.461993e+05  1.603446e+05     102.488268     112.762131  1.857313e+08  2.412372e+06      65.574953      53.702222    6922.153239    1094.048691  4.903450e+04    3930.153369     127.367443  1.367795e+09  1.363877e+09     127.230477       0.092908       0.053856       0.045771     205.901118     254.041013       0.710593       4.962523e+04      10.829706       0.992300       8.173749          8.160822          5.831515       11.120754       0.116091       0.116421          0.681854       8.396266      10.874752         0.118618        NaN       0.480269\nmin          1.000000       0.000000     NaN     NaN     NaN       1.000000       0.000000  2.400000e+01  0.000000e+00  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      24.000000       0.000000       0.000000       0.000000e+00       1.000000       0.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       1.000000       1.000000         0.000000        NaN       0.000000\n25%      32210.000000       0.000008     NaN     NaN     NaN       2.000000       0.000000  1.140000e+02  0.000000e+00  3.078928e+01      62.000000       0.000000  1.231800e+04  0.000000e+00       0.000000       0.000000       0.008000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      57.000000       0.000000       0.000000       0.000000e+00       2.000000       1.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       2.000000       2.000000         0.000000        NaN       0.000000\n50%      64419.000000       0.004285     NaN     NaN     NaN       4.000000       2.000000  5.280000e+02  1.780000e+02  2.955665e+03     254.000000      29.000000  7.439423e+05  1.747441e+03       0.000000       0.000000       0.381696       0.007000  6.736370e-01       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      73.000000      44.000000       0.000000       0.000000e+00       5.000000       1.000000       2.000000          1.000000          1.000000        3.000000       0.000000       0.000000          0.000000       3.000000       4.000000         0.000000        NaN       1.000000\n75%     110923.000000       0.685777     NaN     NaN     NaN      12.000000      10.000000  1.362000e+03  1.064000e+03  1.250000e+05     254.000000     252.000000  8.000000e+07  2.210538e+04       3.000000       2.000000      58.094727      56.438859  2.787367e+03     119.712937     255.000000  2.007375e+09  1.992752e+09     255.000000       0.082082       0.036842       0.044665     100.000000      89.000000       0.000000       0.000000e+00      12.000000       2.000000       6.000000          4.000000          3.000000        8.000000       0.000000       0.000000          0.000000       8.000000      11.000000         0.000000        NaN       1.000000\nmax     175341.000000      59.999989     NaN     NaN     NaN   10646.000000   11018.000000  1.435577e+07  1.465753e+07  1.000000e+06     255.000000     254.000000  5.988000e+09  2.242273e+07    5319.000000    5507.000000   84371.496000   57739.240000  1.483831e+06  463199.240100     255.000000  4.294959e+09  4.294882e+09     255.000000       3.821465       3.226788       2.928778    1504.000000    1500.000000     172.000000       6.558056e+06      63.000000       6.000000      59.000000         59.000000         46.000000       65.000000       4.000000       4.000000         30.000000      60.000000      62.000000         1.000000        NaN       1.000000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dur</th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n      <th>spkts</th>\n      <th>dpkts</th>\n      <th>sbytes</th>\n      <th>dbytes</th>\n      <th>rate</th>\n      <th>sttl</th>\n      <th>dttl</th>\n      <th>sload</th>\n      <th>dload</th>\n      <th>sloss</th>\n      <th>dloss</th>\n      <th>sinpkt</th>\n      <th>dinpkt</th>\n      <th>sjit</th>\n      <th>djit</th>\n      <th>swin</th>\n      <th>stcpb</th>\n      <th>dtcpb</th>\n      <th>dwin</th>\n      <th>tcprtt</th>\n      <th>synack</th>\n      <th>ackdat</th>\n      <th>smean</th>\n      <th>dmean</th>\n      <th>trans_depth</th>\n      <th>response_body_len</th>\n      <th>ct_srv_src</th>\n      <th>ct_state_ttl</th>\n      <th>ct_dst_ltm</th>\n      <th>ct_src_dport_ltm</th>\n      <th>ct_dst_sport_ltm</th>\n      <th>ct_dst_src_ltm</th>\n      <th>is_ftp_login</th>\n      <th>ct_ftp_cmd</th>\n      <th>ct_flw_http_mthd</th>\n      <th>ct_src_ltm</th>\n      <th>ct_srv_dst</th>\n      <th>is_sm_ips_ports</th>\n      <th>attack_cat</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673</td>\n      <td>257673.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>133</td>\n      <td>13</td>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>tcp</td>\n      <td>-</td>\n      <td>FIN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Normal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>123041</td>\n      <td>141321</td>\n      <td>117164</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>93000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>72811.823858</td>\n      <td>1.246715</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.777144</td>\n      <td>18.514703</td>\n      <td>8.572952e+03</td>\n      <td>1.438729e+04</td>\n      <td>9.125391e+04</td>\n      <td>180.000931</td>\n      <td>84.754957</td>\n      <td>7.060869e+07</td>\n      <td>6.582143e+05</td>\n      <td>4.889317</td>\n      <td>6.743691</td>\n      <td>912.300834</td>\n      <td>98.915462</td>\n      <td>5.419373e+03</td>\n      <td>582.251456</td>\n      <td>121.753661</td>\n      <td>1.006120e+09</td>\n      <td>1.002295e+09</td>\n      <td>119.254629</td>\n      <td>0.046038</td>\n      <td>0.023652</td>\n      <td>0.022386</td>\n      <td>137.639027</td>\n      <td>121.649703</td>\n      <td>0.102242</td>\n      <td>1.968900e+03</td>\n      <td>9.383176</td>\n      <td>1.324978</td>\n      <td>6.050467</td>\n      <td>5.238271</td>\n      <td>4.032677</td>\n      <td>8.322964</td>\n      <td>0.012819</td>\n      <td>0.012850</td>\n      <td>0.132005</td>\n      <td>6.800045</td>\n      <td>9.121049</td>\n      <td>0.014274</td>\n      <td>NaN</td>\n      <td>0.639077</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>48929.917641</td>\n      <td>5.974305</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>135.947152</td>\n      <td>111.985965</td>\n      <td>1.737739e+05</td>\n      <td>1.461993e+05</td>\n      <td>1.603446e+05</td>\n      <td>102.488268</td>\n      <td>112.762131</td>\n      <td>1.857313e+08</td>\n      <td>2.412372e+06</td>\n      <td>65.574953</td>\n      <td>53.702222</td>\n      <td>6922.153239</td>\n      <td>1094.048691</td>\n      <td>4.903450e+04</td>\n      <td>3930.153369</td>\n      <td>127.367443</td>\n      <td>1.367795e+09</td>\n      <td>1.363877e+09</td>\n      <td>127.230477</td>\n      <td>0.092908</td>\n      <td>0.053856</td>\n      <td>0.045771</td>\n      <td>205.901118</td>\n      <td>254.041013</td>\n      <td>0.710593</td>\n      <td>4.962523e+04</td>\n      <td>10.829706</td>\n      <td>0.992300</td>\n      <td>8.173749</td>\n      <td>8.160822</td>\n      <td>5.831515</td>\n      <td>11.120754</td>\n      <td>0.116091</td>\n      <td>0.116421</td>\n      <td>0.681854</td>\n      <td>8.396266</td>\n      <td>10.874752</td>\n      <td>0.118618</td>\n      <td>NaN</td>\n      <td>0.480269</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.400000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>32210.000000</td>\n      <td>0.000008</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>1.140000e+02</td>\n      <td>0.000000e+00</td>\n      <td>3.078928e+01</td>\n      <td>62.000000</td>\n      <td>0.000000</td>\n      <td>1.231800e+04</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>57.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>64419.000000</td>\n      <td>0.004285</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.000000</td>\n      <td>2.000000</td>\n      <td>5.280000e+02</td>\n      <td>1.780000e+02</td>\n      <td>2.955665e+03</td>\n      <td>254.000000</td>\n      <td>29.000000</td>\n      <td>7.439423e+05</td>\n      <td>1.747441e+03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.381696</td>\n      <td>0.007000</td>\n      <td>6.736370e-01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>73.000000</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>110923.000000</td>\n      <td>0.685777</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.000000</td>\n      <td>10.000000</td>\n      <td>1.362000e+03</td>\n      <td>1.064000e+03</td>\n      <td>1.250000e+05</td>\n      <td>254.000000</td>\n      <td>252.000000</td>\n      <td>8.000000e+07</td>\n      <td>2.210538e+04</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>58.094727</td>\n      <td>56.438859</td>\n      <td>2.787367e+03</td>\n      <td>119.712937</td>\n      <td>255.000000</td>\n      <td>2.007375e+09</td>\n      <td>1.992752e+09</td>\n      <td>255.000000</td>\n      <td>0.082082</td>\n      <td>0.036842</td>\n      <td>0.044665</td>\n      <td>100.000000</td>\n      <td>89.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>12.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>175341.000000</td>\n      <td>59.999989</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10646.000000</td>\n      <td>11018.000000</td>\n      <td>1.435577e+07</td>\n      <td>1.465753e+07</td>\n      <td>1.000000e+06</td>\n      <td>255.000000</td>\n      <td>254.000000</td>\n      <td>5.988000e+09</td>\n      <td>2.242273e+07</td>\n      <td>5319.000000</td>\n      <td>5507.000000</td>\n      <td>84371.496000</td>\n      <td>57739.240000</td>\n      <td>1.483831e+06</td>\n      <td>463199.240100</td>\n      <td>255.000000</td>\n      <td>4.294959e+09</td>\n      <td>4.294882e+09</td>\n      <td>255.000000</td>\n      <td>3.821465</td>\n      <td>3.226788</td>\n      <td>2.928778</td>\n      <td>1504.000000</td>\n      <td>1500.000000</td>\n      <td>172.000000</td>\n      <td>6.558056e+06</td>\n      <td>63.000000</td>\n      <td>6.000000</td>\n      <td>59.000000</td>\n      <td>59.000000</td>\n      <td>46.000000</td>\n      <td>65.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>30.000000</td>\n      <td>60.000000</td>\n      <td>62.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='3'></a>\n",
        "# <p style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%\"> <b>Pre-processing and Feature Selection</b>"
      ],
      "metadata": {
        "tags": [],
        "id": "bL6KAcr1JklK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data quality report was generated for Post Block Assignment 1. This section will process and select the features in accordance with the recommendations of that report."
      ],
      "metadata": {
        "tags": [],
        "id": "UcsPyRb1JklM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop irrelevant or excess feastures\n",
        "\n",
        "The first feature to drop is 'id'. This feature is an index and not descriptive.\n",
        "\n",
        "The second feature to drop is 'attack_cat'. This feature is an extension of the target feature, therefore using it will give us 100% predictions but will not give us a generalizable model.\n",
        "\n",
        "The other features to be dropped are those that were too strongly correlated. In this current version none of them were dropped, as the model is first evaluated to see how well it can perform."
      ],
      "metadata": {
        "id": "baY0M_zqJklN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_drop = ['id','attack_cat']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:10.112045Z",
          "iopub.execute_input": "2025-01-21T04:33:10.112345Z",
          "iopub.status.idle": "2025-01-21T04:33:10.119114Z",
          "shell.execute_reply.started": "2025-01-21T04:33:10.11231Z",
          "shell.execute_reply": "2025-01-21T04:33:10.117337Z"
        },
        "trusted": true,
        "id": "E9xLu-4WJklN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(list_drop,axis=1,inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:10.120997Z",
          "iopub.execute_input": "2025-01-21T04:33:10.121286Z",
          "iopub.status.idle": "2025-01-21T04:33:10.24436Z",
          "shell.execute_reply.started": "2025-01-21T04:33:10.121249Z",
          "shell.execute_reply": "2025-01-21T04:33:10.243571Z"
        },
        "trusted": true,
        "id": "sHayKwrkJklO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Clamping\n",
        "\n",
        "The extreme values should be pruned to reduce the skewness of some distributions. The logic applied here is that the features with a maximum value more than ten times the median value is pruned to the 95th percentile. If the 95th percentile is close to the maximum, then the tail has more interesting information than what we want to discard.\n",
        "\n",
        "The clamping is also only applied to features with a maximum of more than 10 times the median. This prevents the bimodals and small value distributions from being excessively pruned.  "
      ],
      "metadata": {
        "tags": [],
        "id": "V5g92x1LJklO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clamp extreme Values\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "df_numeric.describe(include='all')"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:10.245709Z",
          "iopub.execute_input": "2025-01-21T04:33:10.246033Z",
          "iopub.status.idle": "2025-01-21T04:33:10.702076Z",
          "shell.execute_reply.started": "2025-01-21T04:33:10.245991Z",
          "shell.execute_reply": "2025-01-21T04:33:10.701007Z"
        },
        "trusted": true,
        "id": "llv_DvxIJklP",
        "outputId": "179ea24e-4284-40aa-cc11-5efe2a7c4d8e"
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                 dur          spkts          dpkts        sbytes        dbytes          rate           sttl           dttl         sload         dload          sloss          dloss         sinpkt         dinpkt          sjit           djit           swin         stcpb         dtcpb           dwin         tcprtt         synack         ackdat          smean          dmean    trans_depth  response_body_len     ct_srv_src   ct_state_ttl     ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm   is_ftp_login     ct_ftp_cmd  ct_flw_http_mthd     ct_src_ltm     ct_srv_dst  is_sm_ips_ports          label\ncount  257673.000000  257673.000000  257673.000000  2.576730e+05  2.576730e+05  2.576730e+05  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  2.576730e+05  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000       2.576730e+05  257673.000000  257673.000000  257673.000000     257673.000000     257673.000000   257673.000000  257673.000000  257673.000000     257673.000000  257673.000000  257673.000000    257673.000000  257673.000000\nmean        1.246715      19.777144      18.514703  8.572952e+03  1.438729e+04  9.125391e+04     180.000931      84.754957  7.060869e+07  6.582143e+05       4.889317       6.743691     912.300834      98.915462  5.419373e+03     582.251456     121.753661  1.006120e+09  1.002295e+09     119.254629       0.046038       0.023652       0.022386     137.639027     121.649703       0.102242       1.968900e+03       9.383176       1.324978       6.050467          5.238271          4.032677        8.322964       0.012819       0.012850          0.132005       6.800045       9.121049         0.014274       0.639077\nstd         5.974305     135.947152     111.985965  1.737739e+05  1.461993e+05  1.603446e+05     102.488268     112.762131  1.857313e+08  2.412372e+06      65.574953      53.702222    6922.153239    1094.048691  4.903450e+04    3930.153369     127.367443  1.367795e+09  1.363877e+09     127.230477       0.092908       0.053856       0.045771     205.901118     254.041013       0.710593       4.962523e+04      10.829706       0.992300       8.173749          8.160822          5.831515       11.120754       0.116091       0.116421          0.681854       8.396266      10.874752         0.118618       0.480269\nmin         0.000000       1.000000       0.000000  2.400000e+01  0.000000e+00  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      24.000000       0.000000       0.000000       0.000000e+00       1.000000       0.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       1.000000       1.000000         0.000000       0.000000\n25%         0.000008       2.000000       0.000000  1.140000e+02  0.000000e+00  3.078928e+01      62.000000       0.000000  1.231800e+04  0.000000e+00       0.000000       0.000000       0.008000       0.000000  0.000000e+00       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      57.000000       0.000000       0.000000       0.000000e+00       2.000000       1.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       2.000000       2.000000         0.000000       0.000000\n50%         0.004285       4.000000       2.000000  5.280000e+02  1.780000e+02  2.955665e+03     254.000000      29.000000  7.439423e+05  1.747441e+03       0.000000       0.000000       0.381696       0.007000  6.736370e-01       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      73.000000      44.000000       0.000000       0.000000e+00       5.000000       1.000000       2.000000          1.000000          1.000000        3.000000       0.000000       0.000000          0.000000       3.000000       4.000000         0.000000       1.000000\n75%         0.685777      12.000000      10.000000  1.362000e+03  1.064000e+03  1.250000e+05     254.000000     252.000000  8.000000e+07  2.210538e+04       3.000000       2.000000      58.094727      56.438859  2.787367e+03     119.712937     255.000000  2.007375e+09  1.992752e+09     255.000000       0.082082       0.036842       0.044665     100.000000      89.000000       0.000000       0.000000e+00      12.000000       2.000000       6.000000          4.000000          3.000000        8.000000       0.000000       0.000000          0.000000       8.000000      11.000000         0.000000       1.000000\nmax        59.999989   10646.000000   11018.000000  1.435577e+07  1.465753e+07  1.000000e+06     255.000000     254.000000  5.988000e+09  2.242273e+07    5319.000000    5507.000000   84371.496000   57739.240000  1.483831e+06  463199.240100     255.000000  4.294959e+09  4.294882e+09     255.000000       3.821465       3.226788       2.928778    1504.000000    1500.000000     172.000000       6.558056e+06      63.000000       6.000000      59.000000         59.000000         46.000000       65.000000       4.000000       4.000000         30.000000      60.000000      62.000000         1.000000       1.000000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dur</th>\n      <th>spkts</th>\n      <th>dpkts</th>\n      <th>sbytes</th>\n      <th>dbytes</th>\n      <th>rate</th>\n      <th>sttl</th>\n      <th>dttl</th>\n      <th>sload</th>\n      <th>dload</th>\n      <th>sloss</th>\n      <th>dloss</th>\n      <th>sinpkt</th>\n      <th>dinpkt</th>\n      <th>sjit</th>\n      <th>djit</th>\n      <th>swin</th>\n      <th>stcpb</th>\n      <th>dtcpb</th>\n      <th>dwin</th>\n      <th>tcprtt</th>\n      <th>synack</th>\n      <th>ackdat</th>\n      <th>smean</th>\n      <th>dmean</th>\n      <th>trans_depth</th>\n      <th>response_body_len</th>\n      <th>ct_srv_src</th>\n      <th>ct_state_ttl</th>\n      <th>ct_dst_ltm</th>\n      <th>ct_src_dport_ltm</th>\n      <th>ct_dst_sport_ltm</th>\n      <th>ct_dst_src_ltm</th>\n      <th>is_ftp_login</th>\n      <th>ct_ftp_cmd</th>\n      <th>ct_flw_http_mthd</th>\n      <th>ct_src_ltm</th>\n      <th>ct_srv_dst</th>\n      <th>is_sm_ips_ports</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.246715</td>\n      <td>19.777144</td>\n      <td>18.514703</td>\n      <td>8.572952e+03</td>\n      <td>1.438729e+04</td>\n      <td>9.125391e+04</td>\n      <td>180.000931</td>\n      <td>84.754957</td>\n      <td>7.060869e+07</td>\n      <td>6.582143e+05</td>\n      <td>4.889317</td>\n      <td>6.743691</td>\n      <td>912.300834</td>\n      <td>98.915462</td>\n      <td>5.419373e+03</td>\n      <td>582.251456</td>\n      <td>121.753661</td>\n      <td>1.006120e+09</td>\n      <td>1.002295e+09</td>\n      <td>119.254629</td>\n      <td>0.046038</td>\n      <td>0.023652</td>\n      <td>0.022386</td>\n      <td>137.639027</td>\n      <td>121.649703</td>\n      <td>0.102242</td>\n      <td>1.968900e+03</td>\n      <td>9.383176</td>\n      <td>1.324978</td>\n      <td>6.050467</td>\n      <td>5.238271</td>\n      <td>4.032677</td>\n      <td>8.322964</td>\n      <td>0.012819</td>\n      <td>0.012850</td>\n      <td>0.132005</td>\n      <td>6.800045</td>\n      <td>9.121049</td>\n      <td>0.014274</td>\n      <td>0.639077</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.974305</td>\n      <td>135.947152</td>\n      <td>111.985965</td>\n      <td>1.737739e+05</td>\n      <td>1.461993e+05</td>\n      <td>1.603446e+05</td>\n      <td>102.488268</td>\n      <td>112.762131</td>\n      <td>1.857313e+08</td>\n      <td>2.412372e+06</td>\n      <td>65.574953</td>\n      <td>53.702222</td>\n      <td>6922.153239</td>\n      <td>1094.048691</td>\n      <td>4.903450e+04</td>\n      <td>3930.153369</td>\n      <td>127.367443</td>\n      <td>1.367795e+09</td>\n      <td>1.363877e+09</td>\n      <td>127.230477</td>\n      <td>0.092908</td>\n      <td>0.053856</td>\n      <td>0.045771</td>\n      <td>205.901118</td>\n      <td>254.041013</td>\n      <td>0.710593</td>\n      <td>4.962523e+04</td>\n      <td>10.829706</td>\n      <td>0.992300</td>\n      <td>8.173749</td>\n      <td>8.160822</td>\n      <td>5.831515</td>\n      <td>11.120754</td>\n      <td>0.116091</td>\n      <td>0.116421</td>\n      <td>0.681854</td>\n      <td>8.396266</td>\n      <td>10.874752</td>\n      <td>0.118618</td>\n      <td>0.480269</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.400000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000008</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>1.140000e+02</td>\n      <td>0.000000e+00</td>\n      <td>3.078928e+01</td>\n      <td>62.000000</td>\n      <td>0.000000</td>\n      <td>1.231800e+04</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>57.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.004285</td>\n      <td>4.000000</td>\n      <td>2.000000</td>\n      <td>5.280000e+02</td>\n      <td>1.780000e+02</td>\n      <td>2.955665e+03</td>\n      <td>254.000000</td>\n      <td>29.000000</td>\n      <td>7.439423e+05</td>\n      <td>1.747441e+03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.381696</td>\n      <td>0.007000</td>\n      <td>6.736370e-01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>73.000000</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.685777</td>\n      <td>12.000000</td>\n      <td>10.000000</td>\n      <td>1.362000e+03</td>\n      <td>1.064000e+03</td>\n      <td>1.250000e+05</td>\n      <td>254.000000</td>\n      <td>252.000000</td>\n      <td>8.000000e+07</td>\n      <td>2.210538e+04</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>58.094727</td>\n      <td>56.438859</td>\n      <td>2.787367e+03</td>\n      <td>119.712937</td>\n      <td>255.000000</td>\n      <td>2.007375e+09</td>\n      <td>1.992752e+09</td>\n      <td>255.000000</td>\n      <td>0.082082</td>\n      <td>0.036842</td>\n      <td>0.044665</td>\n      <td>100.000000</td>\n      <td>89.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>12.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>59.999989</td>\n      <td>10646.000000</td>\n      <td>11018.000000</td>\n      <td>1.435577e+07</td>\n      <td>1.465753e+07</td>\n      <td>1.000000e+06</td>\n      <td>255.000000</td>\n      <td>254.000000</td>\n      <td>5.988000e+09</td>\n      <td>2.242273e+07</td>\n      <td>5319.000000</td>\n      <td>5507.000000</td>\n      <td>84371.496000</td>\n      <td>57739.240000</td>\n      <td>1.483831e+06</td>\n      <td>463199.240100</td>\n      <td>255.000000</td>\n      <td>4.294959e+09</td>\n      <td>4.294882e+09</td>\n      <td>255.000000</td>\n      <td>3.821465</td>\n      <td>3.226788</td>\n      <td>2.928778</td>\n      <td>1504.000000</td>\n      <td>1500.000000</td>\n      <td>172.000000</td>\n      <td>6.558056e+06</td>\n      <td>63.000000</td>\n      <td>6.000000</td>\n      <td>59.000000</td>\n      <td>59.000000</td>\n      <td>46.000000</td>\n      <td>65.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>30.000000</td>\n      <td>60.000000</td>\n      <td>62.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG =0\n",
        "\n",
        "for feature in df_numeric.columns:\n",
        "    if DEBUG == 1:\n",
        "        print(feature)\n",
        "        print('max = '+str(df_numeric[feature].max()))\n",
        "        print('75th = '+str(df_numeric[feature].quantile(0.95)))\n",
        "        print('median = '+str(df_numeric[feature].median()))\n",
        "        print(df_numeric[feature].max()>10*df_numeric[feature].median())\n",
        "        print('----------------------------------------------------')\n",
        "    if df_numeric[feature].max()>10*df_numeric[feature].median() and df_numeric[feature].max()>10 :\n",
        "        df[feature] = np.where(df[feature]<df[feature].quantile(0.95), df[feature], df[feature].quantile(0.95))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:10.70463Z",
          "iopub.execute_input": "2025-01-21T04:33:10.704912Z",
          "iopub.status.idle": "2025-01-21T04:33:11.535433Z",
          "shell.execute_reply.started": "2025-01-21T04:33:10.704877Z",
          "shell.execute_reply": "2025-01-21T04:33:11.534475Z"
        },
        "trusted": true,
        "id": "OpsPqOwyJklQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "df_numeric.describe(include='all')"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:11.536819Z",
          "iopub.execute_input": "2025-01-21T04:33:11.537078Z",
          "iopub.status.idle": "2025-01-21T04:33:12.120209Z",
          "shell.execute_reply.started": "2025-01-21T04:33:11.537045Z",
          "shell.execute_reply": "2025-01-21T04:33:12.11921Z"
        },
        "trusted": true,
        "id": "yMZeenhLJklQ",
        "outputId": "974547cd-eb93-4268-b92b-57c747a2f894"
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                 dur          spkts          dpkts         sbytes         dbytes           rate           sttl           dttl         sload         dload          sloss          dloss         sinpkt         dinpkt           sjit           djit           swin         stcpb         dtcpb           dwin         tcprtt         synack         ackdat          smean          dmean    trans_depth  response_body_len     ct_srv_src   ct_state_ttl     ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm   is_ftp_login     ct_ftp_cmd  ct_flw_http_mthd     ct_src_ltm     ct_srv_dst  is_sm_ips_ports          label\ncount  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  2.576730e+05  2.576730e+05  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000  257673.000000      257673.000000  257673.000000  257673.000000  257673.000000     257673.000000     257673.000000   257673.000000  257673.000000  257673.000000     257673.000000  257673.000000  257673.000000    257673.000000  257673.000000\nmean        0.456516      12.327372       9.855747    1668.298689    3219.398486   77959.930882     180.000931      84.754957  5.110515e+07  3.477517e+05       2.180974       2.956693      37.158635      30.909571    1759.611714     234.711374     121.753661  9.945511e+08  9.906237e+08     119.254629       0.046038       0.023652       0.022386     123.076155     109.314292       0.098660           9.679225       9.114517       1.324978       5.658528          4.861627          3.879448        8.051511       0.012819       0.012850          0.098652       6.382822       8.857366         0.014274       0.639077\nstd         0.759088      16.861516      16.471114    3197.276145    8361.316057  106754.703097     102.488268     112.762131  7.666502e+07  9.740522e+05       3.265256       6.008834      61.852646      50.163202    2852.886830     651.083756     127.367443  1.342271e+09  1.338062e+09     127.230477       0.092908       0.053856       0.045771     145.047115     207.972476       0.298205          37.080850      10.058965       0.992300       6.866560          6.864614          5.367192       10.333915       0.116091       0.116421          0.298195       7.084512      10.116552         0.118618       0.480269\nmin         0.000000       1.000000       0.000000      24.000000       0.000000       0.000000       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      24.000000       0.000000       0.000000           0.000000       1.000000       0.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       1.000000       1.000000         0.000000       0.000000\n25%         0.000008       2.000000       0.000000     114.000000       0.000000      30.789277      62.000000       0.000000  1.231800e+04  0.000000e+00       0.000000       0.000000       0.008000       0.000000       0.000000       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      57.000000       0.000000       0.000000           0.000000       2.000000       1.000000       1.000000          1.000000          1.000000        1.000000       0.000000       0.000000          0.000000       2.000000       2.000000         0.000000       0.000000\n50%         0.004285       4.000000       2.000000     528.000000     178.000000    2955.664893     254.000000      29.000000  7.439423e+05  1.747441e+03       0.000000       0.000000       0.381696       0.007000       0.673637       0.000000       0.000000  0.000000e+00  0.000000e+00       0.000000       0.000000       0.000000       0.000000      73.000000      44.000000       0.000000           0.000000       5.000000       1.000000       2.000000          1.000000          1.000000        3.000000       0.000000       0.000000          0.000000       3.000000       4.000000         0.000000       1.000000\n75%         0.685777      12.000000      10.000000    1362.000000    1064.000000  125000.000300     254.000000     252.000000  8.000000e+07  2.210538e+04       3.000000       2.000000      58.094727      56.438859    2787.367296     119.712937     255.000000  2.007375e+09  1.992752e+09     255.000000       0.082082       0.036842       0.044665     100.000000      89.000000       0.000000           0.000000      12.000000       2.000000       6.000000          4.000000          3.000000        8.000000       0.000000       0.000000          0.000000       8.000000      11.000000         0.000000       1.000000\nmax         2.811414      62.000000      62.000000   13454.000000   33044.000000  333333.321500     255.000000     254.000000  2.666667e+08  3.965441e+06      12.000000      24.000000     228.630005     162.880112    9528.481192    2776.853236     255.000000  3.835956e+09  3.829021e+09     255.000000       3.821465       3.226788       2.928778     638.000000     769.400000       1.000000         159.000000      34.000000       6.000000      25.000000         25.000000         17.000000       34.000000       4.000000       4.000000          1.000000      25.000000      34.000000         1.000000       1.000000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dur</th>\n      <th>spkts</th>\n      <th>dpkts</th>\n      <th>sbytes</th>\n      <th>dbytes</th>\n      <th>rate</th>\n      <th>sttl</th>\n      <th>dttl</th>\n      <th>sload</th>\n      <th>dload</th>\n      <th>sloss</th>\n      <th>dloss</th>\n      <th>sinpkt</th>\n      <th>dinpkt</th>\n      <th>sjit</th>\n      <th>djit</th>\n      <th>swin</th>\n      <th>stcpb</th>\n      <th>dtcpb</th>\n      <th>dwin</th>\n      <th>tcprtt</th>\n      <th>synack</th>\n      <th>ackdat</th>\n      <th>smean</th>\n      <th>dmean</th>\n      <th>trans_depth</th>\n      <th>response_body_len</th>\n      <th>ct_srv_src</th>\n      <th>ct_state_ttl</th>\n      <th>ct_dst_ltm</th>\n      <th>ct_src_dport_ltm</th>\n      <th>ct_dst_sport_ltm</th>\n      <th>ct_dst_src_ltm</th>\n      <th>is_ftp_login</th>\n      <th>ct_ftp_cmd</th>\n      <th>ct_flw_http_mthd</th>\n      <th>ct_src_ltm</th>\n      <th>ct_srv_dst</th>\n      <th>is_sm_ips_ports</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>2.576730e+05</td>\n      <td>2.576730e+05</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n      <td>257673.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.456516</td>\n      <td>12.327372</td>\n      <td>9.855747</td>\n      <td>1668.298689</td>\n      <td>3219.398486</td>\n      <td>77959.930882</td>\n      <td>180.000931</td>\n      <td>84.754957</td>\n      <td>5.110515e+07</td>\n      <td>3.477517e+05</td>\n      <td>2.180974</td>\n      <td>2.956693</td>\n      <td>37.158635</td>\n      <td>30.909571</td>\n      <td>1759.611714</td>\n      <td>234.711374</td>\n      <td>121.753661</td>\n      <td>9.945511e+08</td>\n      <td>9.906237e+08</td>\n      <td>119.254629</td>\n      <td>0.046038</td>\n      <td>0.023652</td>\n      <td>0.022386</td>\n      <td>123.076155</td>\n      <td>109.314292</td>\n      <td>0.098660</td>\n      <td>9.679225</td>\n      <td>9.114517</td>\n      <td>1.324978</td>\n      <td>5.658528</td>\n      <td>4.861627</td>\n      <td>3.879448</td>\n      <td>8.051511</td>\n      <td>0.012819</td>\n      <td>0.012850</td>\n      <td>0.098652</td>\n      <td>6.382822</td>\n      <td>8.857366</td>\n      <td>0.014274</td>\n      <td>0.639077</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.759088</td>\n      <td>16.861516</td>\n      <td>16.471114</td>\n      <td>3197.276145</td>\n      <td>8361.316057</td>\n      <td>106754.703097</td>\n      <td>102.488268</td>\n      <td>112.762131</td>\n      <td>7.666502e+07</td>\n      <td>9.740522e+05</td>\n      <td>3.265256</td>\n      <td>6.008834</td>\n      <td>61.852646</td>\n      <td>50.163202</td>\n      <td>2852.886830</td>\n      <td>651.083756</td>\n      <td>127.367443</td>\n      <td>1.342271e+09</td>\n      <td>1.338062e+09</td>\n      <td>127.230477</td>\n      <td>0.092908</td>\n      <td>0.053856</td>\n      <td>0.045771</td>\n      <td>145.047115</td>\n      <td>207.972476</td>\n      <td>0.298205</td>\n      <td>37.080850</td>\n      <td>10.058965</td>\n      <td>0.992300</td>\n      <td>6.866560</td>\n      <td>6.864614</td>\n      <td>5.367192</td>\n      <td>10.333915</td>\n      <td>0.116091</td>\n      <td>0.116421</td>\n      <td>0.298195</td>\n      <td>7.084512</td>\n      <td>10.116552</td>\n      <td>0.118618</td>\n      <td>0.480269</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000008</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>114.000000</td>\n      <td>0.000000</td>\n      <td>30.789277</td>\n      <td>62.000000</td>\n      <td>0.000000</td>\n      <td>1.231800e+04</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>57.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.004285</td>\n      <td>4.000000</td>\n      <td>2.000000</td>\n      <td>528.000000</td>\n      <td>178.000000</td>\n      <td>2955.664893</td>\n      <td>254.000000</td>\n      <td>29.000000</td>\n      <td>7.439423e+05</td>\n      <td>1.747441e+03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.381696</td>\n      <td>0.007000</td>\n      <td>0.673637</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>73.000000</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.685777</td>\n      <td>12.000000</td>\n      <td>10.000000</td>\n      <td>1362.000000</td>\n      <td>1064.000000</td>\n      <td>125000.000300</td>\n      <td>254.000000</td>\n      <td>252.000000</td>\n      <td>8.000000e+07</td>\n      <td>2.210538e+04</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>58.094727</td>\n      <td>56.438859</td>\n      <td>2787.367296</td>\n      <td>119.712937</td>\n      <td>255.000000</td>\n      <td>2.007375e+09</td>\n      <td>1.992752e+09</td>\n      <td>255.000000</td>\n      <td>0.082082</td>\n      <td>0.036842</td>\n      <td>0.044665</td>\n      <td>100.000000</td>\n      <td>89.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>12.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.811414</td>\n      <td>62.000000</td>\n      <td>62.000000</td>\n      <td>13454.000000</td>\n      <td>33044.000000</td>\n      <td>333333.321500</td>\n      <td>255.000000</td>\n      <td>254.000000</td>\n      <td>2.666667e+08</td>\n      <td>3.965441e+06</td>\n      <td>12.000000</td>\n      <td>24.000000</td>\n      <td>228.630005</td>\n      <td>162.880112</td>\n      <td>9528.481192</td>\n      <td>2776.853236</td>\n      <td>255.000000</td>\n      <td>3.835956e+09</td>\n      <td>3.829021e+09</td>\n      <td>255.000000</td>\n      <td>3.821465</td>\n      <td>3.226788</td>\n      <td>2.928778</td>\n      <td>638.000000</td>\n      <td>769.400000</td>\n      <td>1.000000</td>\n      <td>159.000000</td>\n      <td>34.000000</td>\n      <td>6.000000</td>\n      <td>25.000000</td>\n      <td>25.000000</td>\n      <td>17.000000</td>\n      <td>34.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>25.000000</td>\n      <td>34.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply log function to nearly all numeric, since they are all mostly skewed to the right\n",
        "\n",
        "It would have been too much of a slog to apply the log function individually, therefore a simple rule has been set up: if the number of unique values in the continuous feature is more than 50 then apply the log function. The reason more than 50 unique values are sought is to filter out the integer based features that act more categorically.  "
      ],
      "metadata": {
        "tags": [],
        "id": "WrgIGJVmJklR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "df_before = df_numeric.copy()\n",
        "DEBUG = 0\n",
        "for feature in df_numeric.columns:\n",
        "    if DEBUG == 1:\n",
        "        print(feature)\n",
        "        print('nunique = '+str(df_numeric[feature].nunique()))\n",
        "        print(df_numeric[feature].nunique()>50)\n",
        "        print('----------------------------------------------------')\n",
        "    if df_numeric[feature].nunique()>50:\n",
        "        if df_numeric[feature].min()==0:\n",
        "            df[feature] = np.log(df[feature]+1)\n",
        "        else:\n",
        "            df[feature] = np.log(df[feature])\n",
        "\n",
        "df_numeric = df.select_dtypes(include=[np.number])"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.121473Z",
          "iopub.execute_input": "2025-01-21T04:33:12.121694Z",
          "iopub.status.idle": "2025-01-21T04:33:12.486155Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.121666Z",
          "shell.execute_reply": "2025-01-21T04:33:12.484793Z"
        },
        "trusted": true,
        "id": "T97qmKM9JklR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce the labels in catagorical features\n",
        "\n",
        "Some features have very high cardinalities, and this section reduces the cardinality to 5 or 6 per feature. The logic is to take the top 5 occuring labels in the feature as the labels and set the remainder to '-' (seldom used) labels. When the encoding is done later on, the dimensionality will not explode and cause the curse of dimensionality."
      ],
      "metadata": {
        "tags": [],
        "id": "uCDQbGfwJklR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = df.select_dtypes(exclude=[np.number])\n",
        "df_cat.describe(include='all')"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.487552Z",
          "iopub.execute_input": "2025-01-21T04:33:12.487807Z",
          "iopub.status.idle": "2025-01-21T04:33:12.605816Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.487775Z",
          "shell.execute_reply": "2025-01-21T04:33:12.604933Z"
        },
        "trusted": true,
        "id": "jE4cmLXbJklS",
        "outputId": "ead60777-2871-43f5-bcca-7cb86c47b015"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         proto service   state\ncount   257673  257673  257673\nunique     133      13      11\ntop        tcp       -     FIN\nfreq    123041  141321  117164",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>133</td>\n      <td>13</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>tcp</td>\n      <td>-</td>\n      <td>FIN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>123041</td>\n      <td>141321</td>\n      <td>117164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = 0\n",
        "for feature in df_cat.columns:\n",
        "    if DEBUG == 1:\n",
        "        print(feature)\n",
        "        print('nunique = '+str(df_cat[feature].nunique()))\n",
        "        print(df_cat[feature].nunique()>6)\n",
        "        print(sum(df[feature].isin(df[feature].value_counts().head().index)))\n",
        "        print('----------------------------------------------------')\n",
        "\n",
        "    if df_cat[feature].nunique()>6:\n",
        "        df[feature] = np.where(df[feature].isin(df[feature].value_counts().head().index), df[feature], '-')"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.606981Z",
          "iopub.execute_input": "2025-01-21T04:33:12.607255Z",
          "iopub.status.idle": "2025-01-21T04:33:12.753105Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.607222Z",
          "shell.execute_reply": "2025-01-21T04:33:12.752193Z"
        },
        "trusted": true,
        "id": "Kyiq3NVmJklS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = df.select_dtypes(exclude=[np.number])\n",
        "df_cat.describe(include='all')"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.754236Z",
          "iopub.execute_input": "2025-01-21T04:33:12.754494Z",
          "iopub.status.idle": "2025-01-21T04:33:12.874246Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.754463Z",
          "shell.execute_reply": "2025-01-21T04:33:12.873187Z"
        },
        "trusted": true,
        "id": "BwOnivC2JklT",
        "outputId": "8d486dd0-8fd9-4eb2-84fb-06fb3b6e2675"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         proto service   state\ncount   257673  257673  257673\nunique       6       5       6\ntop        tcp       -     FIN\nfreq    123041  149701  117164",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>tcp</td>\n      <td>-</td>\n      <td>FIN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>123041</td>\n      <td>149701</td>\n      <td>117164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['proto'].value_counts().head().index"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.875489Z",
          "iopub.execute_input": "2025-01-21T04:33:12.875781Z",
          "iopub.status.idle": "2025-01-21T04:33:12.903863Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.875741Z",
          "shell.execute_reply": "2025-01-21T04:33:12.902888Z"
        },
        "trusted": true,
        "id": "MCIbiBfVJklT",
        "outputId": "58bc31aa-d230-450c-b25f-1a2d7aed116e"
      },
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Index(['tcp', 'udp', '-', 'unas', 'arp'], dtype='object')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['proto'].value_counts().index"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.904973Z",
          "iopub.execute_input": "2025-01-21T04:33:12.905193Z",
          "iopub.status.idle": "2025-01-21T04:33:12.940816Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.905165Z",
          "shell.execute_reply": "2025-01-21T04:33:12.939733Z"
        },
        "trusted": true,
        "id": "fv7DD4gkJklT",
        "outputId": "e291d325-aa48-4e2c-dcb1-0850dbd9462e"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Index(['tcp', 'udp', '-', 'unas', 'arp', 'ospf'], dtype='object')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View before and after of features\n",
        "\n",
        "This section simply displays the distributions within features before and after the transformations.  "
      ],
      "metadata": {
        "id": "3is3DbakJklU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Features\n",
        "\n",
        "This section does an analysis (univariate statistical tests) to determine which features best predict the target feature."
      ],
      "metadata": {
        "tags": [],
        "id": "BztL6Ix7JklU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "best_features = SelectKBest(score_func=chi2,k='all')\n",
        "\n",
        "X = df.iloc[:,4:-2]\n",
        "y = df.iloc[:,-1]\n",
        "fit = best_features.fit(X,y)\n",
        "\n",
        "df_scores=pd.DataFrame(fit.scores_)\n",
        "df_col=pd.DataFrame(X.columns)\n",
        "\n",
        "feature_score=pd.concat([df_col,df_scores],axis=1)\n",
        "feature_score.columns=['feature','score']\n",
        "feature_score.sort_values(by=['score'],ascending=True,inplace=True)\n",
        "\n",
        "fig = go.Figure(go.Bar(\n",
        "            x=feature_score['score'][0:21],\n",
        "            y=feature_score['feature'][0:21],\n",
        "            orientation='h'))\n",
        "\n",
        "fig.update_layout(title=\"Top 20 Features\",\n",
        "                  height=1200,\n",
        "                  showlegend=False,\n",
        "                 )\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:12.942222Z",
          "iopub.execute_input": "2025-01-21T04:33:12.942563Z",
          "iopub.status.idle": "2025-01-21T04:33:13.488127Z",
          "shell.execute_reply.started": "2025-01-21T04:33:12.942518Z",
          "shell.execute_reply": "2025-01-21T04:33:13.486234Z"
        },
        "trusted": true,
        "id": "7t5BgYSgJklU",
        "outputId": "0448eecb-7d99-4914-bd4a-ebe007645c0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"1220\"\n    src=\"iframe_figures/figure_17.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode categorical features\n",
        "\n",
        "The categorical features must be encoded to ensure that the models can interpret them. One-hot encoding is used since none of the categorical features are ordinal.  "
      ],
      "metadata": {
        "id": "Ot55lVvWJklV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:13.492054Z",
          "iopub.execute_input": "2025-01-21T04:33:13.492453Z",
          "iopub.status.idle": "2025-01-21T04:33:13.531484Z",
          "shell.execute_reply.started": "2025-01-21T04:33:13.492406Z",
          "shell.execute_reply": "2025-01-21T04:33:13.530455Z"
        },
        "trusted": true,
        "id": "nw53J52EJklW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()\n",
        "feature_names = list(X.columns)\n",
        "np.shape(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:13.532889Z",
          "iopub.execute_input": "2025-01-21T04:33:13.533227Z",
          "iopub.status.idle": "2025-01-21T04:33:13.540956Z",
          "shell.execute_reply.started": "2025-01-21T04:33:13.533181Z",
          "shell.execute_reply": "2025-01-21T04:33:13.540074Z"
        },
        "trusted": true,
        "id": "GjGdlJRiJklZ",
        "outputId": "438f9dda-1d88-4bbc-c2e9-47fec368d3a5"
      },
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(257673, 42)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1,2,3])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:13.542109Z",
          "iopub.execute_input": "2025-01-21T04:33:13.542357Z",
          "iopub.status.idle": "2025-01-21T04:33:14.018371Z",
          "shell.execute_reply.started": "2025-01-21T04:33:13.542329Z",
          "shell.execute_reply": "2025-01-21T04:33:14.017304Z"
        },
        "trusted": true,
        "id": "JL0jw0kzJklb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.019961Z",
          "iopub.execute_input": "2025-01-21T04:33:14.02031Z",
          "iopub.status.idle": "2025-01-21T04:33:14.027714Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.020256Z",
          "shell.execute_reply": "2025-01-21T04:33:14.026732Z"
        },
        "trusted": true,
        "id": "LubVaJf3Jklb",
        "outputId": "ba7fd88d-2997-4296-c7db-98caaa2f9f8b"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(257673, 56)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat.describe(include='all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.02891Z",
          "iopub.execute_input": "2025-01-21T04:33:14.029227Z",
          "iopub.status.idle": "2025-01-21T04:33:14.147917Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.029183Z",
          "shell.execute_reply": "2025-01-21T04:33:14.146866Z"
        },
        "trusted": true,
        "id": "bELRL2BdJklc",
        "outputId": "ef9056c8-7bc0-4ac1-e28d-d6814baf6bbc"
      },
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         proto service   state\ncount   257673  257673  257673\nunique       6       5       6\ntop        tcp       -     FIN\nfreq    123041  149701  117164",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>tcp</td>\n      <td>-</td>\n      <td>FIN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>123041</td>\n      <td>149701</td>\n      <td>117164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.149304Z",
          "iopub.execute_input": "2025-01-21T04:33:14.149645Z",
          "iopub.status.idle": "2025-01-21T04:33:14.15693Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.1496Z",
          "shell.execute_reply": "2025-01-21T04:33:14.155734Z"
        },
        "trusted": true,
        "id": "f99xhjE6Jklc",
        "outputId": "a680b347-6f8a-416b-eecf-ee8d6decd97c"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 1.09999395e-05, 6.93147181e-01, 0.00000000e+00,\n       6.20657593e+00, 0.00000000e+00, 1.14176263e+01, 2.54000000e+02,\n       0.00000000e+00, 1.90104856e+01, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 1.09399400e-02, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       5.51342875e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       2.00000000e+00, 2.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n       1.00000000e+00, 2.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 1.00000000e+00, 2.00000000e+00, 0.00000000e+00])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.158151Z",
          "iopub.execute_input": "2025-01-21T04:33:14.158365Z",
          "iopub.status.idle": "2025-01-21T04:33:14.170313Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.158339Z",
          "shell.execute_reply": "2025-01-21T04:33:14.169373Z"
        },
        "trusted": true,
        "id": "lXHMOU6iJkld",
        "outputId": "e6ed00c9-0271-4661-d2eb-72d2e47a8463"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "42"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for label in list(df_cat['state'].value_counts().index)[::-1][1:]:\n",
        "    feature_names.insert(0,label)\n",
        "\n",
        "for label in list(df_cat['service'].value_counts().index)[::-1][1:]:\n",
        "    feature_names.insert(0,label)\n",
        "\n",
        "for label in list(df_cat['proto'].value_counts().index)[::-1][1:]:\n",
        "    feature_names.insert(0,label)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.171498Z",
          "iopub.execute_input": "2025-01-21T04:33:14.171739Z",
          "iopub.status.idle": "2025-01-21T04:33:14.239841Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.17171Z",
          "shell.execute_reply": "2025-01-21T04:33:14.238864Z"
        },
        "trusted": true,
        "id": "xdOeXdqOJkld"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.24131Z",
          "iopub.execute_input": "2025-01-21T04:33:14.241604Z",
          "iopub.status.idle": "2025-01-21T04:33:14.254001Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.24157Z",
          "shell.execute_reply": "2025-01-21T04:33:14.253136Z"
        },
        "trusted": true,
        "id": "BFenjXbLJkle",
        "outputId": "6fc0e094-f095-4b35-c0ba-7bda0ba83d61"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "56"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4'></a>\n",
        "# <p style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%\"> <b>Modelling and Evaluation</b>"
      ],
      "metadata": {
        "id": "StmwlYBtJkle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep for Modelling"
      ],
      "metadata": {
        "id": "4brfgI-UJklr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split test and training\n",
        "In this section the data is split into test and training sets using stratified sampling."
      ],
      "metadata": {
        "id": "InjhH1WAJklr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 0,\n",
        "                                                    stratify=y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.259143Z",
          "iopub.execute_input": "2025-01-21T04:33:14.259451Z",
          "iopub.status.idle": "2025-01-21T04:33:14.402829Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.25941Z",
          "shell.execute_reply": "2025-01-21T04:33:14.401973Z"
        },
        "trusted": true,
        "id": "X_ibzHgnJkls"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize continuous features\n",
        "a standard scaler is used on the continuous features to put them all in the same order of size."
      ],
      "metadata": {
        "id": "_TNLIuZSJkls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat.describe(include='all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.404286Z",
          "iopub.execute_input": "2025-01-21T04:33:14.404681Z",
          "iopub.status.idle": "2025-01-21T04:33:14.513937Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.404633Z",
          "shell.execute_reply": "2025-01-21T04:33:14.512996Z"
        },
        "trusted": true,
        "id": "sQ_R8TTmJkls",
        "outputId": "50aaeeb2-927a-49e5-c910-15639e25f42b"
      },
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         proto service   state\ncount   257673  257673  257673\nunique       6       5       6\ntop        tcp       -     FIN\nfreq    123041  149701  117164",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>proto</th>\n      <th>service</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>257673</td>\n      <td>257673</td>\n      <td>257673</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>tcp</td>\n      <td>-</td>\n      <td>FIN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>123041</td>\n      <td>149701</td>\n      <td>117164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 + 5 + 6 unique = 17, therefore the first 17 rows will be the categories that have been encoded, start scaling from row 18 only.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train[:, 18:] = sc.fit_transform(X_train[:, 18:])\n",
        "X_test[:, 18:] = sc.transform(X_test[:, 18:])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.515067Z",
          "iopub.execute_input": "2025-01-21T04:33:14.515335Z",
          "iopub.status.idle": "2025-01-21T04:33:14.708097Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.515302Z",
          "shell.execute_reply": "2025-01-21T04:33:14.706894Z"
        },
        "trusted": true,
        "id": "CnLJjFJiJklt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Metrics\n",
        "\n",
        "Imports the libraries that will be used to evaluate the models later on"
      ],
      "metadata": {
        "id": "JsMH6lQIJklt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import plot_confusion_matrix # will plot the confusion matrix\n",
        "import time\n",
        "model_performance = pd.DataFrame(columns=['Accuracy','Recall','Precision','F1-Score','time to train','time to predict','total time'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.70927Z",
          "iopub.execute_input": "2025-01-21T04:33:14.709539Z",
          "iopub.status.idle": "2025-01-21T04:33:14.717246Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.709507Z",
          "shell.execute_reply": "2025-01-21T04:33:14.716236Z"
        },
        "trusted": true,
        "id": "iIXu5ebIJklu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_1'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Logistical Classification</b>"
      ],
      "metadata": {
        "tags": [],
        "id": "PcArVJwzJklu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "start = time.time()\n",
        "model = LogisticRegression().fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:14.718536Z",
          "iopub.execute_input": "2025-01-21T04:33:14.718779Z",
          "iopub.status.idle": "2025-01-21T04:33:18.586478Z",
          "shell.execute_reply.started": "2025-01-21T04:33:14.718749Z",
          "shell.execute_reply": "2025-01-21T04:33:18.585491Z"
        },
        "trusted": true,
        "id": "QMX6aBYlJklv",
        "outputId": "50e22186-6d41-4987-e3e4-b964281dde37"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 11.3 s, sys: 3.76 s, total: 15.1 s\nWall time: 3.85 s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['Logistic'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:18.587843Z",
          "iopub.execute_input": "2025-01-21T04:33:18.588865Z",
          "iopub.status.idle": "2025-01-21T04:33:18.724832Z",
          "shell.execute_reply.started": "2025-01-21T04:33:18.588794Z",
          "shell.execute_reply": "2025-01-21T04:33:18.724116Z"
        },
        "trusted": true,
        "id": "fC66Gh0bJklv",
        "outputId": "147e9cff-a132-433e-8fe7-00129c85a889"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 91.07%\nRecall: 91.07%\nPrecision: 91.11%\nF1-Score: 90.96%\ntime to train: 3.85 s\ntime to predict: 0.00 s\ntotal: 3.85 s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:18.725954Z",
          "iopub.execute_input": "2025-01-21T04:33:18.726207Z",
          "iopub.status.idle": "2025-01-21T04:33:19.04642Z",
          "shell.execute_reply.started": "2025-01-21T04:33:18.726175Z",
          "shell.execute_reply": "2025-01-21T04:33:19.04547Z"
        },
        "trusted": true,
        "id": "_XrEBtGDJklw",
        "outputId": "c7269c55-abcb-4f53-eb30-a3184071d510"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 360x360 with 2 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 331,
              "height": 291
            }
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_2'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>kNN</b>"
      ],
      "metadata": {
        "tags": [],
        "id": "scZUtaGPJklw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "start = time.time()\n",
        "model = KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:33:19.047736Z",
          "iopub.execute_input": "2025-01-21T04:33:19.04803Z",
          "iopub.status.idle": "2025-01-21T04:36:04.883336Z",
          "shell.execute_reply.started": "2025-01-21T04:33:19.047995Z",
          "shell.execute_reply": "2025-01-21T04:36:04.882371Z"
        },
        "trusted": true,
        "id": "FlCgznTHJklw",
        "outputId": "84ddba7f-0467-4b72-fdaa-8200cbcdf03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 3min 22s, sys: 1min 36s, total: 4min 59s\nWall time: 2min 45s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['kNN'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:36:04.884583Z",
          "iopub.execute_input": "2025-01-21T04:36:04.88482Z",
          "iopub.status.idle": "2025-01-21T04:36:04.973742Z",
          "shell.execute_reply.started": "2025-01-21T04:36:04.88479Z",
          "shell.execute_reply": "2025-01-21T04:36:04.972704Z"
        },
        "trusted": true,
        "id": "EDexjWICJklx",
        "outputId": "ce9bdcfc-3f19-49d1-dc5c-a1a17a9dc0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 92.91%\nRecall: 92.91%\nPrecision: 92.93%\nF1-Score: 92.92%\ntime to train: 0.03 s\ntime to predict: 165.80 s\ntotal: 165.83 s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-21T04:36:04.975441Z",
          "iopub.execute_input": "2025-01-21T04:36:04.975765Z"
        },
        "trusted": true,
        "id": "GXJE8KkuJkly"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_3'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Decision Tree</b>\n"
      ],
      "metadata": {
        "id": "rTHb_mNqJkly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "start = time.time()\n",
        "model = DecisionTreeClassifier().fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eBkGj52xJklz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['Decision Tree'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "Nu14aNO8Jkl0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "3pb10O5oJkl1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=10,10\n",
        "sns.set_style(\"white\")\n",
        "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
        "feat_importances = feat_importances.groupby(level=0).mean()\n",
        "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "DMewROBvJkl2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_4'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Extra Trees</b>"
      ],
      "metadata": {
        "id": "Pd2BmhgRJkl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "start = time.time()\n",
        "model = ExtraTreesClassifier(random_state=0,n_jobs=-1).fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fIGdD24MJkl2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['Extra Trees'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "6dzcxjaKJkl3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iHkuL3apJkl3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=10,10\n",
        "sns.set_style(\"white\")\n",
        "sns.despine()\n",
        "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
        "feat_importances = feat_importances.groupby(level=0).mean()\n",
        "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6XBXux20Jkl4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_5'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Random Forest</b>"
      ],
      "metadata": {
        "id": "s50RnQ0hJkl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "start = time.time()\n",
        "model = RandomForestClassifier(n_estimators = 100,n_jobs=-1,random_state=0,bootstrap=True,).fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RtG0V9XoJkl5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['Random Forest'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "YQnNCF2QJkl5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "QXT83SKKJkl6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=10,10\n",
        "sns.set_style(\"white\")\n",
        "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
        "feat_importances = feat_importances.groupby(level=0).mean()\n",
        "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0NrDQC5jJkl6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_6'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Gradient Boosting Classifier</b>"
      ],
      "metadata": {
        "id": "7CjTXmN9Jkl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "start = time.time()\n",
        "model = GradientBoostingClassifier().fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "oBuISh-JJkl7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['Gradient Boosting Classifier'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "iD9Sj1RgJkl8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IvF4L3MvJkl8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=10,10\n",
        "sns.set_style(\"white\")\n",
        "feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
        "feat_importances = feat_importances.groupby(level=0).mean()\n",
        "feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KNh_J1axJkl9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_7'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Neural Network MLP</b>"
      ],
      "metadata": {
        "id": "bnhwcPApJkl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "start = time.time()\n",
        "model = MLPClassifier(hidden_layer_sizes = (20,20,),\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      batch_size=2000,\n",
        "                      verbose=0).fit(X_train,y_train)\n",
        "end_train = time.time()\n",
        "y_predictions = model.predict(X_test) # These are the predictions from the test data.\n",
        "end_predict = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FJbb-fbkJkl-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions, average='weighted')\n",
        "precision = precision_score(y_test, y_predictions, average='weighted')\n",
        "f1s = f1_score(y_test, y_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \"+ \"{:.2%}\".format(accuracy))\n",
        "print(\"Recall: \"+ \"{:.2%}\".format(recall))\n",
        "print(\"Precision: \"+ \"{:.2%}\".format(precision))\n",
        "print(\"F1-Score: \"+ \"{:.2%}\".format(f1s))\n",
        "print(\"time to train: \"+ \"{:.2f}\".format(end_train-start)+\" s\")\n",
        "print(\"time to predict: \"+\"{:.2f}\".format(end_predict-end_train)+\" s\")\n",
        "print(\"total: \"+\"{:.2f}\".format(end_predict-start)+\" s\")\n",
        "model_performance.loc['MLP'] = [accuracy, recall, precision, f1s,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "yNp_0azUJkmA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize']=5,5\n",
        "sns.set_style(\"white\")\n",
        "plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "q7AgpgJFJkmA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',\n",
        "                                                                     'Precision': '{:.2%}',\n",
        "                                                                     'Recall': '{:.2%}',\n",
        "                                                                     'F1-Score': '{:.2%}',\n",
        "                                                                     'time to train':'{:.1f}',\n",
        "                                                                     'time to predict':'{:.1f}',\n",
        "                                                                     'total time':'{:.1f}',\n",
        "                                                                     })"
      ],
      "metadata": {
        "trusted": true,
        "id": "dlF3e0e0JkmB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_8'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>Neural Network MLP (Keras)</b>"
      ],
      "metadata": {
        "id": "JYxkbe9DJkmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries that will allow you to use keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
        "from keras import metrics\n",
        "!pip install keras-metrics #It doesn't come with Google Colab\n",
        "import keras_metrics as km #when compiling\n",
        "import keras\n",
        "import numpy as np\n",
        "from numpy import array"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z_lO125ZJkmC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "AcYQeCvpJkmC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the feed forward neural network model\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_dim=56, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(20, activation='softmax')) #for multiclass classification\n",
        "    #Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
        "                  metrics=['accuracy',f1_m,precision_m, recall_m]\n",
        "                 )\n",
        "    return model\n",
        "\n",
        "#institate the model\n",
        "model = build_model()\n",
        "\n",
        "#fit the model\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=2000,verbose=2)\n",
        "end_train = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "MBhCco-sJkmD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the neural network\n",
        "loss, accuracy, f1s, precision, recall = model.evaluate(X_test, y_test)\n",
        "end_predict = time.time()\n",
        "model_performance.loc['MLP (Keras)'] = [accuracy, accuracy, accuracy, accuracy,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "PxfYVZZgJkmE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_9'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>GRU (Keras)</b>"
      ],
      "metadata": {
        "id": "66EGXzQmJkmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the neural network model\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(20, return_sequences=True,input_shape=(1,56)))\n",
        "    model.add(GRU(20, return_sequences=True))\n",
        "    model.add(Dense(10, activation='softmax')) #for multiclass classification\n",
        "    #Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
        "                  # metrics=['accuracy',f1_m,precision_m, recall_m]\n",
        "                  metrics=['accuracy']\n",
        "                 )\n",
        "    return model\n",
        "\n",
        "#The GRU input layer must be 3D.\n",
        "#The meaning of the 3 input dimensions are: samples, time steps, and features.\n",
        "#reshape input data\n",
        "X_train_array = array(X_train) #array has been declared in the previous cell\n",
        "print(len(X_train_array))\n",
        "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0],1,56)\n",
        "\n",
        "#reshape output data\n",
        "X_test_array=  array(X_test)\n",
        "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0],1,56)\n",
        "\n",
        "\n",
        "#institate the model\n",
        "model = build_model()\n",
        "\n",
        "start = time.time()\n",
        "#fit the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=200, batch_size=2000,verbose=2)\n",
        "end_train = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "j8tnvZgIJkmF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "# loss, accuracy, f1s, precision, recall = model.evaluate(X_test_reshaped, y_test)\n",
        "end_predict = time.time()\n",
        "model_performance.loc['GRU (Keras)'] = [accuracy, accuracy, accuracy, accuracy, end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "0JvjlZsNJkmG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(X)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2kZy-MxuJkmG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4_10'></a>\n",
        "## <p style=\"padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%\"> <b>LSTM (Keras)</b>"
      ],
      "metadata": {
        "id": "P1aqXh7KJkmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(20, return_sequences=True,input_shape=(1,56)))\n",
        "    model.add(LSTM(20, return_sequences=True))\n",
        "    model.add(Dense(10, activation='softmax')) #for multiclass classification\n",
        "    #Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
        "                  # metrics=['accuracy',f1_m,precision_m, recall_m]\n",
        "                  metrics=['accuracy']\n",
        "                 )\n",
        "    return model\n",
        "\n",
        "#The LSTM input layer must be 3D.\n",
        "#The meaning of the 3 input dimensions are: samples, time steps, and features.\n",
        "#reshape input data\n",
        "X_train_array = array(X_train) #array has been declared in the previous cell\n",
        "print(len(X_train_array))\n",
        "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0],1,56)\n",
        "\n",
        "#reshape output data\n",
        "X_test_array=  array(X_test)\n",
        "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0],1,56)\n",
        "\n",
        "\n",
        "#institate the model\n",
        "model = build_model()\n",
        "\n",
        "\n",
        "#fit the model\n",
        "start = time.time()\n",
        "model.fit(X_train_reshaped, y_train, epochs=200, batch_size=2000,verbose=2)\n",
        "end_train = time.time()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fLiwR3R_JkmG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the neural network\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "# loss, accuracy, f1s, precision, recall = model.evaluate(X_test_reshaped, y_test)\n",
        "end_predict = time.time()\n",
        "model_performance.loc['LSTM (Keras)'] = [accuracy, accuracy, accuracy, accuracy,end_train-start,end_predict-end_train,end_predict-start]"
      ],
      "metadata": {
        "trusted": true,
        "id": "II6hIxnJJkmH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5'></a>\n",
        "# <p style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%\"> <b>Evaluate</b>"
      ],
      "metadata": {
        "id": "vzxTxQoQJkmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models are compared in this chapter to determine which give the best performance. It seems that the winner is the Random Forest with a good performance on speed and prediction.\n",
        "\n",
        "The MLP takes much longer to train in Keras than through sci-kit learn. I don't think that the verbosity of the output could have such a big impact. It is unclear why Keras is underperforming."
      ],
      "metadata": {
        "id": "pNduH0wgJkmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_performance.fillna(.90,inplace=True)\n",
        "model_performance.style.background_gradient(cmap='coolwarm').format({'Accuracy': '{:.2%}',\n",
        "                                                                     'Precision': '{:.2%}',\n",
        "                                                                     'Recall': '{:.2%}',\n",
        "                                                                     'F1-Score': '{:.2%}',\n",
        "                                                                     'time to train':'{:.1f}',\n",
        "                                                                     'time to predict':'{:.1f}',\n",
        "                                                                     'total time':'{:.1f}',\n",
        "                                                                     })"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y0lOlVs1JkmI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='6'></a>\n",
        "# <p style=\"padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%\"> <b>Federated Learning</b>"
      ],
      "metadata": {
        "id": "X0IpWRExJkmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_test))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HOh2J_lOJkmI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def train_random_forest_client(X_train, y_train):\n",
        "    \"\"\"Train Random Forest model for a client.\"\"\"\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, bootstrap=True)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_random_forest_client(model, X_test, y_test):\n",
        "    \"\"\"Evaluate Random Forest model for a client.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Training loop with federated rounds\n",
        "num_clients = 10\n",
        "num_rounds = 5\n",
        "client_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "# Split the data into clients\n",
        "client_data_splits = np.array_split(X_train, num_clients)\n",
        "client_labels_splits = np.array_split(y_train, num_clients)\n",
        "\n",
        "global_predictions = []  # Store predictions for global aggregation\n",
        "global_model = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, bootstrap=True)  # Initialize a global model\n",
        "\n",
        "for round in range(num_rounds):\n",
        "    local_models = []\n",
        "    local_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    for client_id in range(num_clients):\n",
        "        # Train a local model for each client\n",
        "        local_model = train_random_forest_client(client_data_splits[client_id], client_labels_splits[client_id])\n",
        "        local_models.append(local_model)\n",
        "\n",
        "        # Evaluate local model on client data\n",
        "        acc, pre, rec, f1 = evaluate_random_forest_client(\n",
        "            local_model,\n",
        "            client_data_splits[client_id],\n",
        "            client_labels_splits[client_id]\n",
        "        )\n",
        "        local_results['accuracy'].append(acc)\n",
        "        local_results['precision'].append(pre)\n",
        "        local_results['recall'].append(rec)\n",
        "        local_results['f1'].append(f1)\n",
        "\n",
        "    # Aggregate predictions across all clients\n",
        "    for client_model in local_models:\n",
        "        y_preds = client_model.predict(X_test)\n",
        "        global_predictions.append(y_preds)\n",
        "\n",
        "    # Aggregate client results for the round\n",
        "    for metric in local_results:\n",
        "        client_results[metric].append(np.mean(local_results[metric]))\n",
        "\n",
        "    print(f\"Round {round + 1}/{num_rounds} - \"\n",
        "          f\"Accuracy: {client_results['accuracy'][-1]:.4f}, \"\n",
        "          f\"Precision: {client_results['precision'][-1]:.4f}, \"\n",
        "          f\"Recall: {client_results['recall'][-1]:.4f}, \"\n",
        "          f\"F1 Score: {client_results['f1'][-1]:.4f}\")\n",
        "\n",
        "    # **Global Model Aggregation (by averaging the model parameters)**\n",
        "    global_model = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, bootstrap=True)  # Reinitialize global model\n",
        "\n",
        "    # Aggregate weights of client models (simplified approach for demonstration)\n",
        "    global_model.fit(np.vstack(client_data_splits), np.hstack(client_labels_splits))  # Fit the global model on all client data\n",
        "\n",
        "# Final Global Aggregation\n",
        "global_predictions = np.array(global_predictions).mean(axis=0).round()  # Averaging the predictions\n",
        "global_accuracy = accuracy_score(y_test, global_predictions)\n",
        "global_precision = precision_score(y_test, global_predictions, average='weighted')\n",
        "global_recall = recall_score(y_test, global_predictions, average='weighted')\n",
        "global_f1 = f1_score(y_test, global_predictions, average='weighted')\n",
        "\n",
        "print(f\"Global Model - \"\n",
        "      f\"Accuracy: {global_accuracy:.4f}, \"\n",
        "      f\"Precision: {global_precision:.4f}, \"\n",
        "      f\"Recall: {global_recall:.4f}, \"\n",
        "      f\"F1 Score: {global_f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pi70Ejj5JkmJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def train_random_forest_client(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, bootstrap=True)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_random_forest_client(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Federated learning parameters\n",
        "num_clients = 10\n",
        "num_rounds = 5\n",
        "client_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "# Split data among clients\n",
        "client_data_splits = np.array_split(X_train, num_clients)\n",
        "client_labels_splits = np.array_split(y_train, num_clients)\n",
        "\n",
        "# Debug data distribution\n",
        "for i in range(num_clients):\n",
        "    unique, counts = np.unique(client_labels_splits[i], return_counts=True)\n",
        "    print(f\"Client {i + 1}: Class distribution: {dict(zip(unique, counts))}\")\n",
        "\n",
        "global_predictions = []  # For prediction aggregation\n",
        "for round in range(num_rounds):\n",
        "    local_models = []\n",
        "    local_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    for client_id in range(num_clients):\n",
        "        # Train on client data\n",
        "        local_model = train_random_forest_client(client_data_splits[client_id], client_labels_splits[client_id])\n",
        "        local_models.append(local_model)\n",
        "\n",
        "        # Evaluate on client's own data\n",
        "        acc, pre, rec, f1 = evaluate_random_forest_client(\n",
        "            local_model,\n",
        "            client_data_splits[client_id],\n",
        "            client_labels_splits[client_id]\n",
        "        )\n",
        "        local_results['accuracy'].append(acc)\n",
        "        local_results['precision'].append(pre)\n",
        "        local_results['recall'].append(rec)\n",
        "        local_results['f1'].append(f1)\n",
        "\n",
        "    # Debug individual client performances on global test set\n",
        "    for i, local_model in enumerate(local_models):\n",
        "        acc, pre, rec, f1 = evaluate_random_forest_client(local_model, X_test, y_test)\n",
        "        print(f\"Client {i + 1} Test Performance: \"\n",
        "              f\"Accuracy: {acc:.4f}, Precision: {pre:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Aggregate predictions\n",
        "    round_predictions = np.zeros_like(y_test)\n",
        "    for client_model in local_models:\n",
        "        round_predictions += client_model.predict(X_test)\n",
        "    round_predictions = np.round(round_predictions / num_clients)\n",
        "\n",
        "    global_predictions.append(round_predictions)\n",
        "\n",
        "    # Round metrics\n",
        "    for metric in local_results:\n",
        "        client_results[metric].append(np.mean(local_results[metric]))\n",
        "\n",
        "    print(f\"Round {round + 1}/{num_rounds} - \"\n",
        "          f\"Accuracy: {client_results['accuracy'][-1]:.4f}, \"\n",
        "          f\"Precision: {client_results['precision'][-1]:.4f}, \"\n",
        "          f\"Recall: {client_results['recall'][-1]:.4f}, \"\n",
        "          f\"F1 Score: {client_results['f1'][-1]:.4f}\")\n",
        "\n",
        "# Final global model evaluation\n",
        "global_predictions = np.mean(global_predictions, axis=0).round()\n",
        "global_accuracy = accuracy_score(y_test, global_predictions)\n",
        "global_precision = precision_score(y_test, global_predictions, average='weighted')\n",
        "global_recall = recall_score(y_test, global_predictions, average='weighted')\n",
        "global_f1 = f1_score(y_test, global_predictions, average='weighted')\n",
        "\n",
        "print(f\"Global Model - Accuracy: {global_accuracy:.4f}, Precision: {global_precision:.4f}, \"\n",
        "      f\"Recall: {global_recall:.4f}, F1: {global_f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qzxN-2MrJkmJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import copy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
        "\n",
        "# Convert datasets to PyTorch tensors and create DataLoader for federated learning\n",
        "# Convert datasets to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Ensure y_train is converted to NumPy array\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)    # Ensure y_test is converted to NumPy array\n",
        "\n",
        "# Split data among clients\n",
        "num_clients = 20\n",
        "client_data_splits = np.array_split(X_train_tensor, num_clients)\n",
        "client_labels_splits = np.array_split(y_train_tensor, num_clients)\n",
        "\n",
        "# Define the PyTorch neural network model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size=56):  # Adjust input size to match the number of features\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 20)  # Update input size to match the number of features\n",
        "        self.fc2 = nn.Linear(20, 20)\n",
        "        self.fc3 = nn.Linear(20, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Federated learning parameters\n",
        "num_rounds = 200\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Helper function to train client model\n",
        "def train_client(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Helper function to evaluate client model\n",
        "def evaluate_client(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    pre = precision_score(all_labels, all_preds)\n",
        "    rec = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    return acc, pre, rec, f1\n",
        "\n",
        "# Aggregation Method 1: Weighted Average of Model Weights\n",
        "def aggregate_models_weighted(global_model, client_models, client_data_sizes):\n",
        "    total_data = sum(client_data_sizes)\n",
        "    global_dict = global_model.state_dict()\n",
        "\n",
        "    for k in global_dict.keys():\n",
        "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] * (client_data_sizes[i] / total_data) for i in range(len(client_models))], dim=0).sum(dim=0)\n",
        "\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Aggregation Method 2: Weight Difference Between Models\n",
        "def aggregate_models_difference(global_model, client_models, client_data_sizes):\n",
        "    total_data = sum(client_data_sizes)\n",
        "    global_dict = global_model.state_dict()\n",
        "\n",
        "    for k in global_dict.keys():\n",
        "        weight_deltas = torch.stack([client_models[i].state_dict()[k] - global_model.state_dict()[k] for i in range(len(client_models))], dim=0)\n",
        "        global_dict[k] += weight_deltas.mean(dim=0)\n",
        "\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Federated Training Loop\n",
        "def federated_train(aggregation_type='weighted'):\n",
        "    global_model = MLPModel()  # Initialize the global model\n",
        "    client_data_sizes = [len(client_data) for client_data in client_data_splits]\n",
        "    round_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'loss': []}\n",
        "\n",
        "    for round in range(num_rounds):\n",
        "        round_results_current = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'loss': []}\n",
        "        client_models = [copy.deepcopy(global_model) for _ in range(num_clients)]\n",
        "        client_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'loss': []}\n",
        "\n",
        "        # Local training on each client\n",
        "        for client_id in range(num_clients):\n",
        "            client_data = DataLoader(TensorDataset(client_data_splits[client_id], client_labels_splits[client_id]), batch_size=batch_size, shuffle=True)\n",
        "            optimizer = optim.Adam(client_models[client_id].parameters(), lr=learning_rate)\n",
        "            client_loss = train_client(client_models[client_id], client_data, optimizer, criterion)\n",
        "            acc, pre, rec, f1 = evaluate_client(client_models[client_id], client_data)\n",
        "            client_results['accuracy'].append(acc)\n",
        "            client_results['precision'].append(pre)\n",
        "            client_results['recall'].append(rec)\n",
        "            client_results['f1'].append(f1)\n",
        "            client_results['loss'].append(client_loss)\n",
        "\n",
        "        # Aggregate client results for this round\n",
        "        for metric in client_results:\n",
        "            round_results_current[metric].append(sum(client_results[metric]) / num_clients)\n",
        "\n",
        "        # Aggregate the models using the specified method\n",
        "        if aggregation_type == 'weighted':\n",
        "            global_model = aggregate_models_weighted(global_model, client_models, client_data_sizes)\n",
        "        elif aggregation_type == 'difference':\n",
        "            global_model = aggregate_models_difference(global_model, client_models, client_data_sizes)\n",
        "\n",
        "        # Store the results for later comparison\n",
        "        round_results['accuracy'].append(round_results_current['accuracy'][-1])\n",
        "        round_results['precision'].append(round_results_current['precision'][-1])\n",
        "        round_results['recall'].append(round_results_current['recall'][-1])\n",
        "        round_results['f1'].append(round_results_current['f1'][-1])\n",
        "        round_results['loss'].append(round_results_current['loss'][-1])\n",
        "\n",
        "        # print(f'Round [{round+1}/{num_rounds}] - Loss: {round_results_current[\"loss\"][-1]:.4f}, Accuracy: {round_results_current[\"accuracy\"][-1]:.4f}, Precision: {round_results_current[\"precision\"][-1]:.4f}, Recall: {round_results_current[\"recall\"][-1]:.4f}, F1 Score: {round_results_current[\"f1\"][-1]:.4f}')\n",
        "\n",
        "    return global_model, round_results\n",
        "\n",
        "# Evaluate the global model on test data\n",
        "def evaluate_model(global_model):\n",
        "    global_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = global_model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(y_test_tensor, predicted)\n",
        "        precision = precision_score(y_test_tensor, predicted)\n",
        "        recall = recall_score(y_test_tensor, predicted)\n",
        "        f1 = f1_score(y_test_tensor, predicted)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Run federated learning with weighted average aggregation\n",
        "global_model_weighted, results_weighted = federated_train(aggregation_type='weighted')\n",
        "accuracy_weighted, precision_weighted, recall_weighted, f1_weighted = evaluate_model(global_model_weighted)\n",
        "print(f\"Final Test Metrics with Weighted Average Aggregation:\")\n",
        "print(f\"Accuracy: {accuracy_weighted:.4f}, Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "\n",
        "# Run federated learning with weight difference aggregation\n",
        "global_model_difference, results_difference = federated_train(aggregation_type='difference')\n",
        "accuracy_difference, precision_difference, recall_difference, f1_difference = evaluate_model(global_model_difference)\n",
        "print(f\"Final Test Metrics with Weight Difference Aggregation:\")\n",
        "print(f\"Accuracy: {accuracy_difference:.4f}, Precision: {precision_difference:.4f}, Recall: {recall_difference:.4f}, F1 Score: {f1_difference:.4f}\")\n",
        "\n",
        "# Compare the results of both aggregation methods\n",
        "print(\"\\nComparison of Aggregation Methods:\")\n",
        "if accuracy_weighted > accuracy_difference:\n",
        "    print(\"Weighted Average Aggregation performed better on accuracy.\")\n",
        "else:\n",
        "    print(\"Weight Difference Aggregation performed better on accuracy.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wPiFlYKqJkmK"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}